{
 "cells": [
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
     "# Sentence Level Aspect-Based Sentiment Analysis with TreeLSTMs\n",
     "\n",
     "This notebook trains a Constituency Tree-LSTM model on the Laptop review dataset using Tensorflow Fold."
   
   ]
  },
 
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "import tensorflow_fold as td\n",
    "import gensim\n",
    "import numpy as np\n",
    "import math"
   ]
  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading & Preprocessing\n",
    "\n",
    "We load the tree strings from the tree folder and convert them into tree objects. The list of tree objects is passed to the main model for training/evaluation.\n",
    "\n",
    "The code in the cell below creates tree objects and provides utilities to operate over them."
    ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    
    "class Node:  # a node in the tree\n",
    "    def __init__(self, label = None, word=None):\n",
    "        self.label = label\n",
    "        self.word = word\n",
    "        self.parent = None  # reference to parent\n",
    "        self.left = None  # reference to left child\n",
    "        self.right = None  # reference to right child\n",
    "        # true if I am a leaf (could have probably derived this from if I have\n",
    "        # a word)\n",
    "        self.isLeaf = False\n",
    "        # true if we have finished performing fowardprop on this node (note,\n",
    "        # there are many ways to implement the recursion.. some might not\n",
    "        # require this flag)\n",
    "        self.level = 0\n",
    "        #defeault intitialziation of depth\n",
    "        self.has_label = False\n",
    "           \n",
    "class Tree:\n",
    "\n",
    "    def __init__(self, treeString, openChar='(', closeChar=')', label_size = 18):\n",
    "        tokens = []\n",
    "        self.open = '('\n",
    "        self.close = ')'\n",
    "        for toks in treeString.strip().split():\n",
    "            tokens += list(toks)\n",
    "        self.root = self.parse(tokens, label_size = label_size)\n",
    "          \n",
    "        self.self_binarize() #ensure binary parse tree - a node can have 0 or 2 child nodes\n",
    "        self.binary = check_for_binarization(self.root)\n",
    "        assert self.binary == True, \"Tree is not binary\"\n",
    "        self.depth = get_depth(self.root)\n",
    "        self.levels = max(math.floor(math.log(float(self.depth)) / math.log(float(2)))-1, 0)\n",
    "        self.labels = get_labels(self.root)\n",
    "       \n",
    "\n",
    "    def parse(self, tokens, parent=None, label_size = 18):\n",
    "        \n",
    "        assert tokens[0] == self.open, \"Malformed tree\"\n",
    "        assert tokens[-1] == self.close, \"Malformed tree\"\n",
    "        \n",
    "        split = 1  # position after open \n",
    "        marker  = 1\n",
    "        countOpen = countClose = 0\n",
    "        label = None\n",
    "        if (split + label_size) < len(tokens):\n",
    "         str1 = ''.join(tokens[split: (split + label_size)])\n",
    "         if str1.isdigit():\n",
    "        \n",
    "            label = tokens[split: (split + label_size)]\n",
    "            label = np.asarray(label).astype(int)\n",
    "            split += label_size\n",
    "            marker += label_size \n",
    "                \n",
    "        if tokens[split] == self.open:\n",
    "            countOpen += 1\n",
    "            split += 1\n",
    "        # Find where left child and right child split\n",
    "        while countOpen != countClose:\n",
    "            if tokens[split] == self.open:\n",
    "                countOpen += 1\n",
    "            if tokens[split] == self.close:\n",
    "                countClose += 1\n",
    "            split += 1\n",
    "\n",
    "        # New node\n",
    "        if isinstance(label, np.ndarray):\n",
    "         node = Node(label)  \n",
    "         node.has_label = True\n",
    "        else:\n",
    "         node = Node()   \n",
    "        \n",
    "        if parent: \n",
    "         node.parent = parent\n",
    "         node.level = parent.level + 1\n",
    "\n",
    "        # leaf Node\n",
    "        if countOpen == 0:\n",
    "            node.word = ''.join(tokens[marker:-1])  # distinguish between lower and upper. Important for words like Apple\n",
    "            node.isLeaf = True\n",
    "            return node\n",
    "\n",
    "        node.left = self.parse(tokens[marker:split], parent=node)\n",
    "        if  (tokens[split] == self.open) :\n",
    "         node.right = self.parse(tokens[split:-1], parent=node)\n",
    "\n",
    "        return node\n",
    "     \n",
    "    def get_words(self):\n",
    "      def get_leaves(node):\n",
    "        if node is None:\n",
    "         return []\n",
    "        if node.isLeaf:\n",
    "         return [node]\n",
    "        else:\n",
    "         return getLeaves(node.left) + getLeaves(node.right)\n",
    "      leaves = getLeaves(self.root)\n",
    "      words = [node.word for node in leaves]\n",
    "      return words\n",
    "\n",
    "\n",
    "     \n",
    "    def self_binarize(self):\n",
    "     \n",
    "     def binarize_tree(node):\n",
    "      \n",
    "      if node.isLeaf:\n",
    "       return\n",
    "      elif ((node.left is not None) & (node.right is not None)):\n",
    "       binarize_tree(node.left)\n",
    "       binarize_tree(node.right)\n",
    "      else:\n",
    "       #fuse parent node with child node\n",
    "       node.left.label = node.label\n",
    "       node.left.level -= 1\n",
    "       \n",
    "       if (node.level != 0):\n",
    "        if (node.parent.right is node):\n",
    "          node.parent.right = node.left\n",
    "        else:\n",
    "          node.parent.left = node.left \n",
    "        node.left.parent = node.parent\n",
    "       \n",
    "       else:\n",
    "        self.root = node.left\n",
    "        node.left.parent = None\n",
    "        self.root.has_label = True\n",
    "       \n",
    "       binarize_tree(node.left)\n",
    "     binarize_tree(self.root)\n",
    "\n",
    "\n",
    " \n",
    "#optional function to push labels to child nodes from root node, Not needed for LSTM trees    \n",
    "def propagate_label(node, levels, depth):\n",
    "    \n",
    "    if node is None:\n",
    "         return\n",
    "    if (node.level > levels):\n",
    "         return\n",
    "    \n",
    "    if node.parent:\n",
    "     node.label = node.parent.label\n",
    "     node.has_label = True\n",
    "    propagate_label(node.left, levels, depth)\n",
    "    propagate_label(node.right, levels, depth)\n",
    "\n",
    "   \n",
    "def get_depth(node):\n",
    "    if node is None:\n",
    "         return\n",
    "\n",
    "    if node.isLeaf:\n",
    "      return 0\n",
    "    return (1+ max(get_depth(node.left), get_depth(node.right)))  \n",
    "\n",
    "\n",
    "def get_labels(node):\n",
    "    if node is None:\n",
    "        return []\n",
    "    if node.has_label == False:\n",
    "        return []\n",
    "    return get_labels(node.left) + get_labels(node.right) + [node.label]\n",
    "\n",
    "\n",
    "\n",
    "def check_for_binarization(node): #check whether we have a binary parse tree\n",
    "      \n",
    "      if node.isLeaf:\n",
    "        return True\n",
    "      elif (node.right is None):\n",
    "        return False \n",
    "      else:\n",
    "       b1 = check_for_binarization(node.left) \n",
    "       b2 = check_for_binarization(node.right)\n",
    "      return (b1 & b2)\n",
    "    "
   ]
  },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
   
    "We load the strings and convert them into a list of tree objects."
    ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadTrees(dataSet='train'):\n",
    "    \"\"\"\n",
    "    Loads training trees. Maps leaf node words to word ids.\n",
    "    \"\"\"\n",
    "    file = 'trees/%s.txt' % dataSet\n",
    "    print (\"Loading %s trees..\" % dataSet)\n",
    "    with open(file, 'r') as fid:\n",
    "        \n",
    "        trees = [Tree(l) for l in fid.readlines()]\n",
    "\n",
    "    return trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train trees..\n",
      "Loading dev trees..\n",
      "Loading test trees..\n"
     ]
    }
   ],
   "source": [
    "train_trees = loadTrees('train')\n",
    "dev_trees = loadTrees('dev')\n",
    "test_trees = loadTrees('test')"
   ]
  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    
    "Create a list of root nodes for each of the tree objects."
    ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
 
   "outputs": [],
   "source": [
    "train_nodes = [t.root for t in train_trees]\n",
    "dev_nodes = [t.root for t in dev_trees]\n",
    "test_nodes = [t.root for t in test_trees]"
   ]
  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    
    "Load the entire Google Word2vec corpus into memory. This will take a few minutes."
    ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadmodel():\n",
    "    print(\"Loading Google Word2vecs....\")\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin.gz', binary = True)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Google Word2vecs....\n"
     ]
    }
   ],
   "source": [
    "model = loadmodel()"
   ]
  },
 {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    
    "Create a dictionary that maps a word to word2vec only for words in the  training, dev and test set."
    ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#only retrain words that are in train, dev and test sets\n",
    "def filter_model(model):\n",
    "    filtered_dict = {}\n",
    "    trees = loadTrees('train') + loadTrees('dev') + loadTrees('test')\n",
    "    words = [t.get_words() for t in trees]\n",
    "    vocab = set()\n",
    "    for word in words:\n",
    "        vocab.update(word)\n",
    "    for word in vocab:\n",
    "        if word in model.vocab:\n",
    "            filtered_dict[word] = model[word]\n",
    "    return filtered_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train trees..\n",
      "Loading dev trees..\n",
      "Loading test trees..\n"
     ]
    }
   ],
   "source": [
    "filtered_model = filter_model(model)"
   ]
  },
   {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    
    "Loads embedings, returns weight matrix and dict from words to indices."
    ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_embeddings(filtered_model):\n",
    "  print('loading word embeddings')\n",
    "  weight_vectors = []\n",
    "  word_idx = {}\n",
    "  for word, vector in filtered_model.items():\n",
    "    word_idx[word] = len(weight_vectors)\n",
    "    weight_vectors.append(np.array(vector, dtype=np.float32))\n",
    "  # Random embedding vector for unknown words.\n",
    "  weight_vectors.append(np.random.uniform(\n",
    "      -0.05, 0.05, weight_vectors[0].shape).astype(np.float32))\n",
    "  return np.stack(weight_vectors), word_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings\n"
     ]
    }
   ],
   "source": [
    "weight_matrix, word_idx = load_embeddings(filtered_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BinaryTreeLSTMCell(tf.contrib.rnn.BasicLSTMCell):\n",
    "  \"\"\"LSTM with two state inputs.\n",
    "\n",
    "  This is the model described in section 3.2 of 'Improved Semantic\n",
    "  Representations From Tree-Structured Long Short-Term Memory\n",
    "  Networks' <http://arxiv.org/pdf/1503.00075.pdf>, with recurrent\n",
    "  dropout as described in 'Recurrent Dropout without Memory Loss'\n",
    "  <http://arxiv.org/pdf/1603.05118.pdf>.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, num_units, keep_prob=1.0):\n",
    "    \"\"\"Initialize the cell.\n",
    "\n",
    "    Args:\n",
    "      num_units: int, The number of units in the LSTM cell.\n",
    "      keep_prob: Keep probability for recurrent dropout.\n",
    "    \"\"\"\n",
    "    super(BinaryTreeLSTMCell, self).__init__(num_units)\n",
    "    self._keep_prob = keep_prob\n",
    "\n",
    "  def __call__(self, inputs, state, scope=None):\n",
    "    with tf.variable_scope(scope or type(self).__name__):\n",
    "      lhs, rhs = state\n",
    "      c0, h0 = lhs\n",
    "      c1, h1 = rhs\n",
    "      concat = tf.contrib.layers.linear(\n",
    "          tf.concat([inputs, h0, h1], 1), 5 * self._num_units)\n",
    "\n",
    "      # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n",
    "      i, j, f0, f1, o = tf.split(value=concat, num_or_size_splits=5, axis=1)\n",
    "\n",
    "      j = self._activation(j)\n",
    "      if not isinstance(self._keep_prob, float) or self._keep_prob < 1:\n",
    "        j = tf.nn.dropout(j, self._keep_prob)\n",
    "\n",
    "      new_c = (c0 * tf.sigmoid(f0 + self._forget_bias) +\n",
    "               c1 * tf.sigmoid(f1 + self._forget_bias) +\n",
    "               tf.sigmoid(i) * j)\n",
    "      new_h = self._activation(new_c) * tf.sigmoid(o)\n",
    "\n",
    "      new_state = tf.contrib.rnn.LSTMStateTuple(new_c, new_h)\n",
    "\n",
    "      return new_h, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob_ph = tf.placeholder_with_default(1.0, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_num_units = 300  # Tai et al. used 150, but our regularization strategy is more effective\n",
    "tree_lstm = td.ScopedLayer(\n",
    "      tf.contrib.rnn.DropoutWrapper(\n",
    "          BinaryTreeLSTMCell(lstm_num_units, keep_prob=keep_prob_ph),\n",
    "          input_keep_prob=keep_prob_ph, output_keep_prob=keep_prob_ph),\n",
    "      name_or_scope='tree_lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_ASPECTS = 18  # number of aspects\n",
    "NUM_POLARITY = 3 #number of polarity classes assicated with an aspect (1 = mildly +ve or -ve, 2 = -ve, 3 = +ve)\n",
    "output_layer = td.FC(NUM_ASPECTS*(NUM_POLARITY+2), activation=None,  name='output_layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embedding = td.Embedding(\n",
    "    *weight_matrix.shape, initializer=weight_matrix, name='word_embedding', trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_subtree = td.ForwardDeclaration(name='embed_subtree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logits_and_state():\n",
    "  \"\"\"Creates a block that goes from tokens to (logits, state) tuples.\"\"\"\n",
    "  unknown_idx = len(word_idx)\n",
    "  lookup_word = lambda word: word_idx.get(word, unknown_idx)\n",
    "  \n",
    "  word2vec = (td.GetItem(0) >> td.InputTransform(lookup_word) >>\n",
    "              td.Scalar('int32') >> word_embedding)\n",
    "\n",
    "  pair2vec = (embed_subtree(), embed_subtree())\n",
    "\n",
    "  # Trees are binary, so the tree layer takes two states as its input_state.\n",
    "  zero_state = td.Zeros((tree_lstm.state_size,) * 2)\n",
    "  # Input is a word vector.\n",
    "  zero_inp = td.Zeros(word_embedding.output_type.shape[0])\n",
    "\n",
    "  word_case = td.AllOf(word2vec, zero_state)\n",
    "  pair_case = td.AllOf(zero_inp, pair2vec)\n",
    "\n",
    "  tree2vec = td.OneOf(len, [(1, word_case), (2, pair_case)])\n",
    "\n",
    "  return tree2vec >> tree_lstm >> (output_layer, td.Identity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_node_loss(logits, labels):\n",
    "  logits_ = tf.reshape(logits, [-1, NUM_ASPECTS,  NUM_POLARITY + 2])\n",
    "  #compute loss related to task 1: aspect detection\n",
    "  binarized = tf.cast((labels > 0), tf.int32) #binarize the labels to compute loss for aspect detection \n",
    "  logits2 = tf.slice(logits_, [0,0,0], [-1,-1, 2])\n",
    "  loss2 = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits2, labels=binarized), axis = 1)\n",
    "\n",
    "  # compute loss related to task 2: polarity prediction \n",
    "  padding = tf.constant([[0,0], [0,0], [1,0]])\n",
    "  logits3 = tf.pad(tf.log(tf.nn.softmax(tf.slice(logits_, [0, 0, 2], [-1,-1, -1]))), padding)\n",
    "  labels2 = tf.pad(tf.slice(tf.one_hot(labels, depth = 4, axis = -1), [0,0,1], [-1,-1,-1]), padding)\n",
    "  loss3 = tf.reduce_sum(tf.multiply(labels2, logits3), [1,2])\n",
    "  final_loss = loss2 + tf.scalar_mul(-1.05,loss3)\n",
    "     \n",
    "  return final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Task 2: compute true positives for aspect polarities\n",
    "def task2_truepositives(logits, labels):\n",
    "    \n",
    "    logits_ = tf.reshape(logits, [-1, NUM_ASPECTS,  NUM_POLARITY + 2])\n",
    "   \n",
    "    predictions = tf.cast(((( logits_[:,:, 2] ) > (logits_[:,:, 3] )) & (( logits_[:,:, 2] ) > (logits_[:,:, 4] ))), tf.float64)\n",
    "    actuals = tf.cast(((labels > 0) & (labels < 2)), tf.float64)\n",
    "   \n",
    "    ones_like_actuals = tf.ones_like(actuals)\n",
    "    zeros_like_actuals = tf.zeros_like(actuals)\n",
    "    ones_like_predictions = tf.ones_like(predictions)\n",
    "    zeros_like_predictions = tf.zeros_like(predictions)\n",
    "\n",
    "    ans_1 = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "           tf.equal(actuals, ones_like_actuals), \n",
    "           tf.equal(predictions, ones_like_predictions)\n",
    "      ), \n",
    "      tf.float64\n",
    "     ), axis = 1\n",
    "    )\n",
    "    \n",
    "    predictions = tf.cast(((( logits_[:,:, 3] ) > (logits_[:,:, 2] )) & (( logits_[:,:, 3] ) > (logits_[:,:, 4] ))), tf.float64)\n",
    "    actuals = tf.cast(((labels > 1) & (labels < 3)), tf.float64)\n",
    " \n",
    "    ones_like_actuals = tf.ones_like(actuals)\n",
    "    zeros_like_actuals = tf.zeros_like(actuals)\n",
    "    ones_like_predictions = tf.ones_like(predictions)\n",
    "    zeros_like_predictions = tf.zeros_like(predictions)\n",
    "\n",
    "    ans_2 = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "           tf.equal(actuals, ones_like_actuals), \n",
    "           tf.equal(predictions, ones_like_predictions)\n",
    "      ), \n",
    "      tf.float64\n",
    "     ), axis = 1\n",
    "    )\n",
    "    \n",
    "    predictions = tf.cast(((( logits_[:,:, 4] ) > (logits_[:,:, 2] )) & (( logits_[:,:, 4] ) > (logits_[:,:, 3] ))), tf.float64)\n",
    "    actuals = tf.cast(((labels > 2) & (labels < 4)), tf.float64)\n",
    "    ones_like_actuals = tf.ones_like(actuals)\n",
    "    zeros_like_actuals = tf.zeros_like(actuals)\n",
    "    ones_like_predictions = tf.ones_like(predictions)\n",
    "    zeros_like_predictions = tf.zeros_like(predictions)\n",
    "\n",
    "    ans_3 = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "           tf.equal(actuals, ones_like_actuals), \n",
    "           tf.equal(predictions, ones_like_predictions)\n",
    "      ), \n",
    "      tf.float64\n",
    "     ), axis = 1\n",
    "    )\n",
    "\n",
    "    return ans_1 + ans_2 + ans_3\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Task 2: compute total number of aspects\n",
    "def task2_dem(logits, labels):\n",
    "    actuals =  tf.cast(labels > 0, tf.float64)\n",
    "    ones_like_actuals = tf.ones_like(actuals)\n",
    "    return tf.reduce_sum(tf.cast(tf.equal(actuals, ones_like_actuals), tf.float64), axis = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Task 1: compute true positive rate\n",
    "def tf_tpr(logits, labels):\n",
    "  logits_ = tf.nn.softmax(tf.reshape(logits, [-1, NUM_ASPECTS, NUM_POLARITY + 2]))\n",
    "  predictions = tf.cast(( logits_[:,:, 1] ) > (logits_[:,:, 0]), tf.float64)\n",
    " \n",
    "  actuals = tf.cast( labels > 0, tf.float64)\n",
    " \n",
    "\n",
    "  ones_like_actuals = tf.ones_like(actuals)\n",
    "  zeros_like_actuals = tf.zeros_like(actuals)\n",
    "  ones_like_predictions = tf.ones_like(predictions)\n",
    "  zeros_like_predictions = tf.zeros_like(predictions)\n",
    "\n",
    "  ans = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "           tf.equal(actuals, ones_like_actuals), \n",
    "           tf.equal(predictions, ones_like_predictions)\n",
    "      ), \n",
    "      tf.float64\n",
    "    ), axis = 1\n",
    "  )\n",
    "\n",
    "  return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Task 1: compute true negative rate\n",
    "def tf_tnr(logits, labels):\n",
    "  logits_ = tf.nn.softmax(tf.reshape(logits, [-1, NUM_ASPECTS, NUM_POLARITY + 2]))\n",
    "\n",
    "  predictions = tf.cast((logits_[:,:, 1] ) > (logits_[:,:, 0]), tf.float64)\n",
    "  actuals = tf.cast(labels > 0, tf.float64)\n",
    "\n",
    "  ones_like_actuals = tf.ones_like(actuals)\n",
    "  zeros_like_actuals = tf.zeros_like(actuals)\n",
    "  ones_like_predictions = tf.ones_like(predictions)\n",
    "  zeros_like_predictions = tf.zeros_like(predictions)\n",
    "\n",
    "  ans = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "           tf.equal(actuals, zeros_like_actuals), \n",
    "           tf.equal(predictions, zeros_like_predictions)\n",
    "      ), \n",
    "      tf.float64\n",
    "    ), axis = 1\n",
    "  )\n",
    "\n",
    "  return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Task 1: compute false positive rate\n",
    "def tf_fpr(logits, labels):\n",
    "  logits_ = tf.nn.softmax(tf.reshape(logits, [-1, NUM_ASPECTS,  NUM_POLARITY + 2]))\n",
    "  predictions = tf.cast((logits_[:,:, 1] ) > (logits_[:,:, 0]), tf.float64)\n",
    "  actuals = tf.cast(labels > 0, tf.float64)\n",
    "\n",
    "  ones_like_actuals = tf.ones_like(actuals)\n",
    "  zeros_like_actuals = tf.zeros_like(actuals)\n",
    "  ones_like_predictions = tf.ones_like(predictions)\n",
    "  zeros_like_predictions = tf.zeros_like(predictions)\n",
    "\n",
    "  ans = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "           tf.equal(actuals, zeros_like_actuals), \n",
    "           tf.equal(predictions, ones_like_predictions)\n",
    "      ), \n",
    "      tf.float64\n",
    "    ), axis = 1\n",
    "  )\n",
    "\n",
    "  return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Task 1: compute false negative rate\n",
    "def tf_fnr(logits, labels):\n",
    "  logits_ = tf.nn.softmax(tf.reshape(logits, [-1, NUM_ASPECTS, NUM_POLARITY + 2]))\n",
    "  predictions = tf.cast((logits_[:,:, 1] ) > (logits_[:,:, 0]), tf.float64)\n",
    "  actuals = tf.cast(labels > 0, tf.float64)\n",
    "\n",
    "  ones_like_actuals = tf.ones_like(actuals)\n",
    "  zeros_like_actuals = tf.zeros_like(actuals)\n",
    "  ones_like_predictions = tf.ones_like(predictions)\n",
    "  zeros_like_predictions = tf.zeros_like(predictions)\n",
    "\n",
    "  ans = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "           tf.equal(actuals, ones_like_actuals), \n",
    "           tf.equal(predictions, zeros_like_predictions)\n",
    "      ), \n",
    "      tf.float64\n",
    "    ), axis = 1\n",
    "  )\n",
    "\n",
    "  return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_metrics(is_root, is_neutral):\n",
    "  \"\"\"A block that adds metrics for loss and hits; output is the LSTM state.\"\"\"\n",
    "  c = td.Composition(\n",
    "      name='predict(is_root=%s, is_neutral=%s)' % (is_root, is_neutral))\n",
    "  with c.scope():\n",
    "    # destructure the input; (labels, neutral, (logits, state))\n",
    "    labels = c.input[0]\n",
    "    logits = td.GetItem(0).reads(c.input[2])\n",
    "    state = td.GetItem(1).reads(c.input[2])\n",
    "\n",
    "    loss = td.Function(tf_node_loss)\n",
    "    td.Metric('all_loss').reads(loss.reads(logits, labels))\n",
    "    if is_root: td.Metric('root_loss').reads(loss)\n",
    "   \n",
    "    tpr = td.Function(tf_tpr)\n",
    "    tnr = td.Function(tf_tnr)\n",
    "    fpr = td.Function(tf_fpr)\n",
    "    fnr = td.Function(tf_fnr)\n",
    "    t2_acc = td.Function(task2_truepositives)\n",
    "    t2_dem = td.Function(task2_dem)\n",
    "    td.Metric('all_tpr').reads(tpr.reads(logits, labels))\n",
    "    td.Metric('all_tnr').reads(tnr.reads(logits, labels))\n",
    "    td.Metric('all_fpr').reads(fpr.reads(logits, labels))\n",
    "    td.Metric('all_fnr').reads(fnr.reads(logits, labels)) \n",
    "    td.Metric('all_task2').reads(t2_acc.reads(logits, labels)) \n",
    "    td.Metric('all_task2dem').reads(t2_dem.reads(logits, labels)) \n",
    "    if is_root: \n",
    "        td.Metric('tpr').reads(tpr)\n",
    "        td.Metric('tnr').reads(tnr)\n",
    "        td.Metric('fpr').reads(fpr)\n",
    "        td.Metric('fnr').reads(fnr)\n",
    "        td.Metric('task2').reads(t2_acc)\n",
    "        td.Metric('task2dem').reads(t2_dem)\n",
    "   \n",
    "    # output the state, which will be read by our by parent's LSTM cell\n",
    "    c.output.reads(state)\n",
    "  return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(node):\n",
    "  group = []\n",
    "  neutral = '2'\n",
    "  if node.has_label:\n",
    "  \n",
    "    label = node.label\n",
    "    \n",
    "    neutral = '1'\n",
    "  else:\n",
    "    label = np.zeros((NUM_ASPECTS,),dtype=np.int)\n",
    "    \n",
    "  if node.isLeaf:\n",
    "    group = [node.word]\n",
    "  else:\n",
    "    group = [node.left, node.right]\n",
    "  return label, neutral, group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(18,)\n"
     ]
    }
   ],
   "source": [
    "node = train_nodes[0]\n",
    "label, neutral, group = tokenize(node)\n",
    "print (len(group))\n",
    "print (label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embed_tree(logits_and_state, is_root):\n",
    "  \"\"\"Creates a block that embeds trees; output is tree LSTM state.\"\"\"\n",
    "  return td.InputTransform(tokenize) >> td.OneOf(\n",
    "      key_fn=lambda pair: pair[1] == '2',  # label 2 means neutral\n",
    "      case_blocks=(add_metrics(is_root, is_neutral=False),\n",
    "                   add_metrics(is_root, is_neutral=True)),\n",
    "      pre_block=(td.Vector(NUM_ASPECTS, dtype = 'int32'), td.Scalar('int32'), logits_and_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = embed_tree(logits_and_state(), is_root=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_subtree.resolve_to(embed_tree(logits_and_state(), is_root=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input type: PyObjectType()\n",
      "output type: TupleType(TensorType((300,), 'float32'), TensorType((300,), 'float32'))\n"
     ]
    }
   ],
   "source": [
    "compiler = td.Compiler.create(model)\n",
    "print('input type: %s' % model.input_type)\n",
    "print('output type: %s' % model.output_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics = {k: tf.reduce_mean(v) for k, v in compiler.metric_tensors.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.05\n",
    "KEEP_PROB = 0.60\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = LEARNING_RATE\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           600, 0.90, staircase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohitgupta/anaconda2/envs/LSTM2/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "train_feed_dict = {keep_prob_ph: KEEP_PROB}\n",
    "loss = tf.reduce_mean(compiler.metric_tensors['root_loss'])\n",
    "opt = tf.train.AdagradOptimizer(LEARNING_RATE)\n",
    "learning_step = (\n",
    "    tf.train.AdagradOptimizer(learning_rate)\n",
    "    .minimize(loss, global_step=global_step)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_step(batch):\n",
    "  train_feed_dict[compiler.loom_input_tensor] = batch\n",
    "  _, batch_loss = sess.run([learning_step, loss], train_feed_dict)\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(train_set):\n",
    "  list = [train_step(batch) for batch in td.group_by_batches(train_set, BATCH_SIZE)]\n",
    "  return sum(list)/ max(len(list), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = compiler.build_loom_inputs(train_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_feed_dict = compiler.build_feed_dict(dev_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dev_eval(epoch, train_loss):\n",
    "  dev_metrics = sess.run(metrics, dev_feed_dict)\n",
    "  dev_loss = dev_metrics['root_loss']\n",
    "\n",
    "  tp = dev_metrics['tpr']\n",
    "  tn = dev_metrics['tnr']\n",
    "  fp = dev_metrics['fpr']\n",
    "  fn = dev_metrics['fnr']\n",
    "  \n",
    "  tpr = float(tp)/(float(tp) + float(fn))\n",
    "  fpr = float(fp)/(float(tp) + float(fn))\n",
    "  t2_acc = float(dev_metrics['task2'])/ float(dev_metrics['task2dem'])\n",
    "\n",
    "  recall = tpr\n",
    "  if (float(tp) + float(fp)) > 0:\n",
    "   precision = float(tp)/(float(tp) + float(fp))\n",
    "  else: precision = 0.\n",
    "  if precision + recall > 0:\n",
    "   f1_score = (2 * (precision * recall)) / (precision + recall)\n",
    "  else: f1_score = 0.\n",
    " \n",
    " \n",
    "  print('epoch:%4d, train_loss: %.3e, dev_loss: %.3e,Task1 Precision: %.3e, Task1 Recall: %.3e, Task1 F1 score: %2.3e, Task2 Acc: %2.3e'\n",
    "        % (epoch, train_loss, dev_loss, precision, recall, f1_score, t2_acc))\n",
    "  return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1, train_loss: 5.386e+00, dev_loss: 5.296e+00,Task1 Precision: 7.143e-01, Task1 Recall: 4.274e-02, Task1 F1 score: 8.065e-02, Task2 Acc: 5.769e-01\n",
      "epoch:   2, train_loss: 4.789e+00, dev_loss: 5.213e+00,Task1 Precision: 5.909e-01, Task1 Recall: 5.556e-02, Task1 F1 score: 1.016e-01, Task2 Acc: 5.812e-01\n",
      "epoch:   3, train_loss: 4.687e+00, dev_loss: 4.907e+00,Task1 Precision: 0.000e+00, Task1 Recall: 0.000e+00, Task1 F1 score: 0.000e+00, Task2 Acc: 6.197e-01\n",
      "epoch:   4, train_loss: 4.580e+00, dev_loss: 4.953e+00,Task1 Precision: 1.000e+00, Task1 Recall: 3.419e-02, Task1 F1 score: 6.612e-02, Task2 Acc: 6.026e-01\n",
      "epoch:   5, train_loss: 4.541e+00, dev_loss: 4.887e+00,Task1 Precision: 6.522e-01, Task1 Recall: 6.410e-02, Task1 F1 score: 1.167e-01, Task2 Acc: 5.598e-01\n",
      "epoch:   6, train_loss: 4.453e+00, dev_loss: 5.142e+00,Task1 Precision: 3.711e-01, Task1 Recall: 1.538e-01, Task1 F1 score: 2.175e-01, Task2 Acc: 5.983e-01\n",
      "epoch:   7, train_loss: 4.342e+00, dev_loss: 4.768e+00,Task1 Precision: 4.505e-01, Task1 Recall: 1.752e-01, Task1 F1 score: 2.523e-01, Task2 Acc: 5.940e-01\n",
      "epoch:   8, train_loss: 4.262e+00, dev_loss: 4.791e+00,Task1 Precision: 7.600e-01, Task1 Recall: 8.120e-02, Task1 F1 score: 1.467e-01, Task2 Acc: 6.239e-01\n",
      "epoch:   9, train_loss: 4.163e+00, dev_loss: 4.563e+00,Task1 Precision: 4.038e-01, Task1 Recall: 8.974e-02, Task1 F1 score: 1.469e-01, Task2 Acc: 6.538e-01\n",
      "epoch:  10, train_loss: 4.080e+00, dev_loss: 4.492e+00,Task1 Precision: 5.610e-01, Task1 Recall: 1.966e-01, Task1 F1 score: 2.911e-01, Task2 Acc: 6.282e-01\n",
      "epoch:  11, train_loss: 3.966e+00, dev_loss: 4.701e+00,Task1 Precision: 4.224e-01, Task1 Recall: 2.094e-01, Task1 F1 score: 2.800e-01, Task2 Acc: 6.838e-01\n",
      "epoch:  12, train_loss: 3.868e+00, dev_loss: 5.899e+00,Task1 Precision: 2.398e-01, Task1 Recall: 2.521e-01, Task1 F1 score: 2.458e-01, Task2 Acc: 6.581e-01\n",
      "epoch:  13, train_loss: 3.818e+00, dev_loss: 4.402e+00,Task1 Precision: 6.615e-01, Task1 Recall: 1.838e-01, Task1 F1 score: 2.876e-01, Task2 Acc: 6.154e-01\n",
      "epoch:  14, train_loss: 3.754e+00, dev_loss: 4.595e+00,Task1 Precision: 4.733e-01, Task1 Recall: 3.034e-01, Task1 F1 score: 3.698e-01, Task2 Acc: 6.282e-01\n",
      "epoch:  15, train_loss: 3.730e+00, dev_loss: 4.331e+00,Task1 Precision: 5.738e-01, Task1 Recall: 1.496e-01, Task1 F1 score: 2.373e-01, Task2 Acc: 6.581e-01\n",
      "epoch:  16, train_loss: 3.636e+00, dev_loss: 4.156e+00,Task1 Precision: 6.866e-01, Task1 Recall: 1.966e-01, Task1 F1 score: 3.056e-01, Task2 Acc: 6.838e-01\n",
      "epoch:  17, train_loss: 3.570e+00, dev_loss: 4.213e+00,Task1 Precision: 6.571e-01, Task1 Recall: 1.966e-01, Task1 F1 score: 3.026e-01, Task2 Acc: 6.496e-01\n",
      "epoch:  18, train_loss: 3.557e+00, dev_loss: 4.749e+00,Task1 Precision: 3.750e-01, Task1 Recall: 3.462e-01, Task1 F1 score: 3.600e-01, Task2 Acc: 6.453e-01\n",
      "epoch:  19, train_loss: 3.454e+00, dev_loss: 4.308e+00,Task1 Precision: 5.422e-01, Task1 Recall: 1.923e-01, Task1 F1 score: 2.839e-01, Task2 Acc: 6.923e-01\n",
      "epoch:  20, train_loss: 3.471e+00, dev_loss: 5.974e+00,Task1 Precision: 2.605e-01, Task1 Recall: 3.462e-01, Task1 F1 score: 2.972e-01, Task2 Acc: 6.624e-01\n",
      "epoch:  21, train_loss: 3.451e+00, dev_loss: 4.334e+00,Task1 Precision: 5.789e-01, Task1 Recall: 2.350e-01, Task1 F1 score: 3.343e-01, Task2 Acc: 6.239e-01\n",
      "epoch:  22, train_loss: 3.383e+00, dev_loss: 4.072e+00,Task1 Precision: 5.182e-01, Task1 Recall: 2.436e-01, Task1 F1 score: 3.314e-01, Task2 Acc: 6.966e-01\n",
      "epoch:  23, train_loss: 3.321e+00, dev_loss: 4.171e+00,Task1 Precision: 4.765e-01, Task1 Recall: 3.034e-01, Task1 F1 score: 3.708e-01, Task2 Acc: 6.752e-01\n",
      "epoch:  24, train_loss: 3.280e+00, dev_loss: 3.875e+00,Task1 Precision: 6.095e-01, Task1 Recall: 2.735e-01, Task1 F1 score: 3.776e-01, Task2 Acc: 6.752e-01\n",
      "epoch:  25, train_loss: 3.210e+00, dev_loss: 3.984e+00,Task1 Precision: 5.574e-01, Task1 Recall: 2.906e-01, Task1 F1 score: 3.820e-01, Task2 Acc: 7.137e-01\n",
      "epoch:  26, train_loss: 3.208e+00, dev_loss: 3.876e+00,Task1 Precision: 6.628e-01, Task1 Recall: 2.436e-01, Task1 F1 score: 3.562e-01, Task2 Acc: 6.581e-01\n",
      "epoch:  27, train_loss: 3.158e+00, dev_loss: 4.113e+00,Task1 Precision: 5.260e-01, Task1 Recall: 3.889e-01, Task1 F1 score: 4.472e-01, Task2 Acc: 6.880e-01\n",
      "epoch:  28, train_loss: 3.148e+00, dev_loss: 3.805e+00,Task1 Precision: 6.216e-01, Task1 Recall: 2.949e-01, Task1 F1 score: 4.000e-01, Task2 Acc: 7.308e-01\n",
      "epoch:  29, train_loss: 3.101e+00, dev_loss: 3.736e+00,Task1 Precision: 5.714e-01, Task1 Recall: 3.419e-01, Task1 F1 score: 4.278e-01, Task2 Acc: 7.393e-01\n",
      "epoch:  30, train_loss: 3.042e+00, dev_loss: 3.651e+00,Task1 Precision: 7.470e-01, Task1 Recall: 2.650e-01, Task1 F1 score: 3.912e-01, Task2 Acc: 7.308e-01\n",
      "epoch:  31, train_loss: 2.996e+00, dev_loss: 3.918e+00,Task1 Precision: 6.355e-01, Task1 Recall: 2.906e-01, Task1 F1 score: 3.988e-01, Task2 Acc: 6.752e-01\n",
      "epoch:  32, train_loss: 2.990e+00, dev_loss: 3.679e+00,Task1 Precision: 7.191e-01, Task1 Recall: 2.735e-01, Task1 F1 score: 3.963e-01, Task2 Acc: 7.350e-01\n",
      "epoch:  33, train_loss: 2.940e+00, dev_loss: 3.773e+00,Task1 Precision: 6.372e-01, Task1 Recall: 3.077e-01, Task1 F1 score: 4.150e-01, Task2 Acc: 7.436e-01\n",
      "epoch:  34, train_loss: 2.931e+00, dev_loss: 3.768e+00,Task1 Precision: 6.465e-01, Task1 Recall: 2.735e-01, Task1 F1 score: 3.844e-01, Task2 Acc: 7.350e-01\n",
      "epoch:  35, train_loss: 2.938e+00, dev_loss: 4.161e+00,Task1 Precision: 5.118e-01, Task1 Recall: 3.718e-01, Task1 F1 score: 4.307e-01, Task2 Acc: 6.752e-01\n",
      "epoch:  36, train_loss: 2.904e+00, dev_loss: 3.745e+00,Task1 Precision: 7.407e-01, Task1 Recall: 2.564e-01, Task1 F1 score: 3.810e-01, Task2 Acc: 7.479e-01\n",
      "epoch:  37, train_loss: 2.829e+00, dev_loss: 3.717e+00,Task1 Precision: 6.435e-01, Task1 Recall: 3.162e-01, Task1 F1 score: 4.241e-01, Task2 Acc: 7.350e-01\n",
      "epoch:  38, train_loss: 2.858e+00, dev_loss: 4.260e+00,Task1 Precision: 5.029e-01, Task1 Recall: 3.718e-01, Task1 F1 score: 4.275e-01, Task2 Acc: 6.239e-01\n",
      "epoch:  39, train_loss: 2.796e+00, dev_loss: 3.671e+00,Task1 Precision: 6.000e-01, Task1 Recall: 3.205e-01, Task1 F1 score: 4.178e-01, Task2 Acc: 7.009e-01\n",
      "epoch:  40, train_loss: 2.800e+00, dev_loss: 3.712e+00,Task1 Precision: 6.061e-01, Task1 Recall: 3.419e-01, Task1 F1 score: 4.372e-01, Task2 Acc: 7.350e-01\n",
      "epoch:  41, train_loss: 2.774e+00, dev_loss: 3.726e+00,Task1 Precision: 5.766e-01, Task1 Recall: 3.376e-01, Task1 F1 score: 4.259e-01, Task2 Acc: 7.051e-01\n",
      "epoch:  42, train_loss: 2.723e+00, dev_loss: 3.627e+00,Task1 Precision: 5.931e-01, Task1 Recall: 3.675e-01, Task1 F1 score: 4.538e-01, Task2 Acc: 7.179e-01\n",
      "epoch:  43, train_loss: 2.636e+00, dev_loss: 3.650e+00,Task1 Precision: 6.496e-01, Task1 Recall: 3.248e-01, Task1 F1 score: 4.330e-01, Task2 Acc: 7.350e-01\n",
      "epoch:  44, train_loss: 2.615e+00, dev_loss: 3.674e+00,Task1 Precision: 6.400e-01, Task1 Recall: 3.419e-01, Task1 F1 score: 4.457e-01, Task2 Acc: 7.009e-01\n",
      "epoch:  45, train_loss: 2.642e+00, dev_loss: 3.791e+00,Task1 Precision: 6.148e-01, Task1 Recall: 3.205e-01, Task1 F1 score: 4.213e-01, Task2 Acc: 7.137e-01\n",
      "epoch:  46, train_loss: 2.630e+00, dev_loss: 3.726e+00,Task1 Precision: 6.016e-01, Task1 Recall: 3.291e-01, Task1 F1 score: 4.254e-01, Task2 Acc: 7.179e-01\n",
      "epoch:  47, train_loss: 2.620e+00, dev_loss: 3.595e+00,Task1 Precision: 5.942e-01, Task1 Recall: 3.504e-01, Task1 F1 score: 4.409e-01, Task2 Acc: 7.137e-01\n",
      "epoch:  48, train_loss: 2.623e+00, dev_loss: 3.986e+00,Task1 Precision: 5.144e-01, Task1 Recall: 4.573e-01, Task1 F1 score: 4.842e-01, Task2 Acc: 7.393e-01\n",
      "epoch:  49, train_loss: 2.590e+00, dev_loss: 3.651e+00,Task1 Precision: 6.557e-01, Task1 Recall: 3.419e-01, Task1 F1 score: 4.494e-01, Task2 Acc: 7.350e-01\n",
      "epoch:  50, train_loss: 2.540e+00, dev_loss: 3.630e+00,Task1 Precision: 5.686e-01, Task1 Recall: 3.718e-01, Task1 F1 score: 4.496e-01, Task2 Acc: 7.350e-01\n",
      "epoch:  51, train_loss: 2.514e+00, dev_loss: 3.655e+00,Task1 Precision: 6.067e-01, Task1 Recall: 3.889e-01, Task1 F1 score: 4.740e-01, Task2 Acc: 7.479e-01\n",
      "epoch:  52, train_loss: 2.517e+00, dev_loss: 3.711e+00,Task1 Precision: 5.988e-01, Task1 Recall: 4.145e-01, Task1 F1 score: 4.899e-01, Task2 Acc: 7.179e-01\n",
      "epoch:  53, train_loss: 2.503e+00, dev_loss: 3.863e+00,Task1 Precision: 5.260e-01, Task1 Recall: 3.462e-01, Task1 F1 score: 4.175e-01, Task2 Acc: 7.564e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  54, train_loss: 2.493e+00, dev_loss: 3.779e+00,Task1 Precision: 6.108e-01, Task1 Recall: 4.359e-01, Task1 F1 score: 5.087e-01, Task2 Acc: 6.795e-01\n",
      "epoch:  55, train_loss: 2.429e+00, dev_loss: 3.688e+00,Task1 Precision: 6.241e-01, Task1 Recall: 3.761e-01, Task1 F1 score: 4.693e-01, Task2 Acc: 7.051e-01\n",
      "epoch:  56, train_loss: 2.431e+00, dev_loss: 3.747e+00,Task1 Precision: 5.445e-01, Task1 Recall: 4.444e-01, Task1 F1 score: 4.894e-01, Task2 Acc: 7.436e-01\n",
      "epoch:  57, train_loss: 2.419e+00, dev_loss: 3.655e+00,Task1 Precision: 6.357e-01, Task1 Recall: 3.803e-01, Task1 F1 score: 4.759e-01, Task2 Acc: 6.923e-01\n",
      "epoch:  58, train_loss: 2.399e+00, dev_loss: 3.595e+00,Task1 Precision: 6.026e-01, Task1 Recall: 4.017e-01, Task1 F1 score: 4.821e-01, Task2 Acc: 7.393e-01\n",
      "epoch:  59, train_loss: 2.426e+00, dev_loss: 3.710e+00,Task1 Precision: 6.119e-01, Task1 Recall: 3.504e-01, Task1 F1 score: 4.457e-01, Task2 Acc: 7.222e-01\n",
      "epoch:  60, train_loss: 2.372e+00, dev_loss: 3.781e+00,Task1 Precision: 6.026e-01, Task1 Recall: 4.017e-01, Task1 F1 score: 4.821e-01, Task2 Acc: 7.350e-01\n",
      "epoch:  61, train_loss: 2.395e+00, dev_loss: 3.750e+00,Task1 Precision: 6.000e-01, Task1 Recall: 4.231e-01, Task1 F1 score: 4.962e-01, Task2 Acc: 7.094e-01\n",
      "epoch:  62, train_loss: 2.347e+00, dev_loss: 3.633e+00,Task1 Precision: 6.288e-01, Task1 Recall: 3.547e-01, Task1 F1 score: 4.536e-01, Task2 Acc: 7.564e-01\n",
      "epoch:  63, train_loss: 2.299e+00, dev_loss: 3.765e+00,Task1 Precision: 6.197e-01, Task1 Recall: 3.761e-01, Task1 F1 score: 4.681e-01, Task2 Acc: 7.778e-01\n",
      "epoch:  64, train_loss: 2.251e+00, dev_loss: 3.787e+00,Task1 Precision: 6.935e-01, Task1 Recall: 3.675e-01, Task1 F1 score: 4.804e-01, Task2 Acc: 7.265e-01\n",
      "epoch:  65, train_loss: 2.313e+00, dev_loss: 3.583e+00,Task1 Precision: 6.145e-01, Task1 Recall: 4.359e-01, Task1 F1 score: 5.100e-01, Task2 Acc: 7.308e-01\n",
      "epoch:  66, train_loss: 2.271e+00, dev_loss: 3.819e+00,Task1 Precision: 7.130e-01, Task1 Recall: 3.291e-01, Task1 F1 score: 4.503e-01, Task2 Acc: 7.308e-01\n",
      "epoch:  67, train_loss: 2.278e+00, dev_loss: 3.578e+00,Task1 Precision: 6.715e-01, Task1 Recall: 3.932e-01, Task1 F1 score: 4.960e-01, Task2 Acc: 7.222e-01\n",
      "epoch:  68, train_loss: 2.237e+00, dev_loss: 3.885e+00,Task1 Precision: 6.857e-01, Task1 Recall: 3.077e-01, Task1 F1 score: 4.248e-01, Task2 Acc: 7.393e-01\n",
      "epoch:  69, train_loss: 2.238e+00, dev_loss: 3.687e+00,Task1 Precision: 6.467e-01, Task1 Recall: 4.145e-01, Task1 F1 score: 5.052e-01, Task2 Acc: 7.393e-01\n",
      "epoch:  70, train_loss: 2.195e+00, dev_loss: 3.667e+00,Task1 Precision: 6.200e-01, Task1 Recall: 3.974e-01, Task1 F1 score: 4.844e-01, Task2 Acc: 7.393e-01\n",
      "epoch:  71, train_loss: 2.152e+00, dev_loss: 3.723e+00,Task1 Precision: 6.531e-01, Task1 Recall: 4.103e-01, Task1 F1 score: 5.039e-01, Task2 Acc: 7.479e-01\n",
      "epoch:  72, train_loss: 2.198e+00, dev_loss: 3.953e+00,Task1 Precision: 6.591e-01, Task1 Recall: 3.718e-01, Task1 F1 score: 4.754e-01, Task2 Acc: 7.479e-01\n",
      "epoch:  73, train_loss: 2.201e+00, dev_loss: 3.686e+00,Task1 Precision: 6.433e-01, Task1 Recall: 4.316e-01, Task1 F1 score: 5.166e-01, Task2 Acc: 7.479e-01\n",
      "epoch:  74, train_loss: 2.177e+00, dev_loss: 3.678e+00,Task1 Precision: 6.503e-01, Task1 Recall: 3.974e-01, Task1 F1 score: 4.934e-01, Task2 Acc: 7.479e-01\n",
      "epoch:  75, train_loss: 2.174e+00, dev_loss: 3.801e+00,Task1 Precision: 6.438e-01, Task1 Recall: 4.017e-01, Task1 F1 score: 4.947e-01, Task2 Acc: 7.436e-01\n",
      "epoch:  76, train_loss: 2.170e+00, dev_loss: 3.752e+00,Task1 Precision: 6.736e-01, Task1 Recall: 4.145e-01, Task1 F1 score: 5.132e-01, Task2 Acc: 7.564e-01\n",
      "epoch:  77, train_loss: 2.154e+00, dev_loss: 3.624e+00,Task1 Precision: 6.690e-01, Task1 Recall: 4.145e-01, Task1 F1 score: 5.119e-01, Task2 Acc: 7.692e-01\n",
      "epoch:  78, train_loss: 2.145e+00, dev_loss: 3.701e+00,Task1 Precision: 6.581e-01, Task1 Recall: 4.359e-01, Task1 F1 score: 5.244e-01, Task2 Acc: 7.436e-01\n",
      "epoch:  79, train_loss: 2.077e+00, dev_loss: 3.808e+00,Task1 Precision: 6.645e-01, Task1 Recall: 4.316e-01, Task1 F1 score: 5.233e-01, Task2 Acc: 7.308e-01\n",
      "epoch:  80, train_loss: 2.126e+00, dev_loss: 3.798e+00,Task1 Precision: 6.522e-01, Task1 Recall: 3.846e-01, Task1 F1 score: 4.839e-01, Task2 Acc: 7.521e-01\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0.0\n",
    "save_path = 'weights/sentiment_model'\n",
    "for epoch, shuffled in enumerate(td.epochs(train_set, EPOCHS), 1):\n",
    "  train_loss = train_epoch(shuffled)\n",
    "  f1_score = dev_eval(epoch, train_loss)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
